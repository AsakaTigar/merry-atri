import re
import json
import glob
import os
import time

# ================= é…ç½® =================
SOURCE_DIR = "/mnt/t2-6tb/Linpeikai/Voice/ATRI/dataset/phase2_import"
OUTPUT_DIR = "/mnt/t2-6tb/Linpeikai/Voice/ATRI/dataset/llm_finetune"
DATASET_INFO_PATH = "/mnt/t2-6tb/Linpeikai/Voice/ATRI/frameworks/LLaMA-Factory/data/dataset_info.json"

# è§’è‰²æ˜ å°„è¡¨
ROLE_MAP = {
    # ATRI (AI)
    "ã‚¢ãƒˆãƒª": "gpt",
    "ATR": "gpt",
    
    # HUMANS (User)
    "å¤ç”Ÿ": "human",
    "NAT": "human",
    "æ°´èœèŒ": "human",
    "MIN": "human", 
    "ã‚­ãƒ£ã‚µãƒªãƒ³": "human",
    "CAT": "human",
    "ç«œå¸": "human",
    "RYU": "human",
    "ãƒªãƒªã‚«": "human",
    "RIR": "human",
    # åªè¦æ˜¯ä¸è®¤è¯†çš„ï¼Œå¦‚æœæœ‰åå­—ï¼Œå¤§æ¦‚ç‡æ˜¯é…è§’ï¼Œè§†ä¸º human
}

def clean_text(text):
    if not text: return ""
    text = text.replace('\\"', '"').replace('\\n', '\n')
    text = re.sub(r'%f[^;]+;', '', text) # å»é™¤å­—ä½“æ ‡è®°
    # å»é™¤å¼•å·
    text = re.sub(r'^[ã€Œ"â€œâ€œ]', '', text)
    text = re.sub(r'[ã€"â€â€]$', '', text)
    return text.strip()

def extract():
    print("ğŸš€ å¼€å§‹æå–å¤šè¯­è¨€å¯¹è¯æ•°æ®...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    files = glob.glob(os.path.join(SOURCE_DIR, "*.json"))
    print(f"ğŸ“‚ æ‰«æ {len(files)} ä¸ªæ–‡ä»¶ in {SOURCE_DIR}")
    
    all_conversations = []
    
    # æ ¸å¿ƒ Regex:
    # åŒ¹é… ["Char", ..., [[JA], [EN], [CN], ...]]
    # æˆ‘ä»¬æ•æ‰ Group 1 (Char) å’Œ Group 2 (CN Text)
    # å¦‚æœ CN ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬æš‚æ—¶ä¹Ÿä¸è¦ JA (å› ä¸ºæˆ‘ä»¬éœ€è¦è®­ç»ƒä¸­æ–‡æ¨¡å‹)
    
    # æ³¨æ„ï¼šJSON é‡Œçš„ç»“æ„æ˜¯ [[null,"JA"], [null,"EN"], [null,"CN"]]
    # è€Œä¸”ä¹‹é—´å¯èƒ½æœ‰æ¢è¡Œï¼Œå› ä¸º grep æ˜¾ç¤ºåœ¨ä¸€è¡Œæ˜¯ grep çš„è¡Œä¸ºï¼Œå®é™…æ–‡ä»¶å³ä½¿è¢«å‹ç¼©æˆä¸€è¡Œï¼ŒRegex ä¹Ÿè¦èƒ½åŒ¹é…ã€‚
    # æˆ‘ä»¬å…ˆè¯»å–æ•´ä¸ªæ–‡ä»¶ï¼Œç„¶åç§»é™¤æ¢è¡Œï¼Œå†åŒ¹é…ã€‚
    
    pattern = re.compile(
        r'\[\s*"([^"]+)"\s*,\s*(?:null|"[^"]*")\s*,\s*\[\s*'       # ["Char", DisplayName, [
        r'\[\s*(?:null|"[^"]*")\s*,\s*"(?:[^"\\]|\\.)*"\s*\]\s*,\s*' # JA
        r'\[\s*(?:null|"[^"]*")\s*,\s*"(?:[^"\\]|\\.)*"\s*\]\s*,\s*' # EN
        r'\[\s*(?:null|"[^"]*")\s*,\s*"((?:[^"\\]|\\.)*)"\s*\]'      # CN -> Group 2
    )
    
    total_found = 0
    
    for fpath in files:
        if fpath.endswith(".resx.json"): continue
        
        try:
            with open(fpath, 'r', encoding='utf-8') as f:
                # æš´åŠ›ç§»é™¤æ¢è¡Œï¼Œç¡®ä¿ Regex èƒ½åœ¨ä¸€è¡Œå†…åŒ¹é…æ‰€æœ‰å†…å®¹
                # è¿™å¯¹äºå¤„ç†æ ¼å¼åŒ–/éæ ¼å¼åŒ–çš„ JSON éƒ½æœ€ç¨³å¥
                content = f.read().replace('\n', ' ') 
        except Exception as e:
            print(f"Skipping {fpath}: {e}")
            continue
            
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…
        matches = pattern.findall(content)
        if not matches:
             # æœ‰äº›æ–‡ä»¶å¯èƒ½åªæœ‰æ—¥æ–‡ï¼Œæ²¡æœ‰ EN/CNï¼Œè¿™äº›æ­£åˆ™ä¼šå¤±è´¥ã€‚
             # ä½†æˆ‘ä»¬è¦çš„æ˜¯ä¸­æ–‡æ•°æ®ã€‚
             continue
             
        # print(f"File {os.path.basename(fpath)}: Found {len(matches)} lines")
        total_found += len(matches)
        
        current_conv = []
        
        for char_id, raw_text in matches:
            text = clean_text(raw_text)
            if not text: continue
            
            # ç¡®å®šè§’è‰²
            role = ROLE_MAP.get(char_id)
            if not role:
                # å¦‚æœä¸åœ¨ Map é‡Œï¼Œä½†ä¹Ÿä¸æ˜¯ "envupdate" è¿™ç§å‘½ä»¤
                # æˆ‘ä»¬å‡è®¾å®ƒæ˜¯é…è§’ human
                # æ’é™¤çº¯æŒ‡ä»¤
                if len(char_id) > 20 or "update" in char_id:
                    continue
                role = "human"
            
            # æ„å»ºå¯¹è¯æµ
            if not current_conv:
                # å¿…é¡»ç”± human/gpt å¼€å¤´ã€‚å¦‚æœç¬¬ä¸€å¥å°±æ˜¯ gptï¼Œæˆ‘ä»¬æ€ä¹ˆå¤„ç†ï¼Ÿ
                # ShareGPT æ ¼å¼æœ€å¥½æ˜¯ human å¼€å¤´ã€‚
                # ä½†å¦‚æœæ˜¯ gpt å¼€å¤´ï¼Œæˆ‘ä»¬å¯ä»¥è¡¥ä¸€ä¸ªç©º humanï¼Œæˆ–è€…å…è®¸ gpt å¼€å¤´ (LLaMA Factory warning)
                current_conv.append({"from": role, "value": text})
            else:
                last_msg = current_conv[-1]
                if last_msg["from"] == role:
                    # åˆå¹¶åŒä¸€ä¸ªäººè¿ç»­å‘è¯
                    last_msg["value"] += " " + text
                else:
                    current_conv.append({"from": role, "value": text})
        
        # ä¿å­˜è¯¥æ–‡ä»¶çš„å¯¹è¯
        if len(current_conv) >= 2:
            # åªæœ‰åŒ…å« GPT çš„å¯¹è¯æ‰æœ‰æ„ä¹‰
            if any(msg["from"] == "gpt" for msg in current_conv):
                all_conversations.append({
                    "conversations": current_conv,
                    "system": "ä½ å«äºšæ‰˜è‰ï¼ˆAtriï¼‰ï¼Œæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æœºå™¨äººå°‘å¥³ã€‚ä½ è¯´è¯è¯­æ°”ç•¥å¸¦éª„å‚²ï¼Œä½†å†…å¿ƒæ¸©æŸ”ã€‚"
                })

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_filename = f"atri_sharegpt_{timestamp}.json"
    output_path = os.path.join(OUTPUT_DIR, output_filename)
    
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"   - åŸå§‹æå–è¡Œæ•°: {total_found}")
    print(f"   - ç”Ÿæˆå¯¹è¯ç»„æ•°: {len(all_conversations)}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_conversations, f, ensure_ascii=False, indent=2)
        
    return output_path, timestamp

if __name__ == "__main__":
    path, ts = extract()
    
    # æ³¨å†Œ
    try:
        with open(DATASET_INFO_PATH, 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        key = f"atri_corpus_{ts}"
        info[key] = {
            "file_name": path,
            "formatting": "sharegpt",
            "columns": {"messages": "conversations", "system": "system"}
        }
        
        with open(DATASET_INFO_PATH, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
            
        # å†™å…¥ key ç»™ bash
        with open("/mnt/t2-6tb/Linpeikai/Voice/ATRI/latest_dataset_key.tmp", "w") as f:
            f.write(key)
            
        print(f"Key registered: {key}")
        
    except Exception as e:
        print(f"Registration failed: {e}")
