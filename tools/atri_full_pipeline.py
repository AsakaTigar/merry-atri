#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ATRI å…¨é“¾è·¯æµ‹è¯•è„šæœ¬
LLM æƒ…æ„Ÿåˆ†æ â†’ å‚è€ƒéŸ³é¢‘é€‰æ‹© â†’ GPT-SoVITS v4 åˆæˆ

ç”¨æ³•: python atri_full_pipeline.py --text "ä½ çš„æ–‡æœ¬"
"""

import os
import sys
import json
import re
import random
import argparse
import numpy as np
import soundfile as sf
from datetime import datetime

# === Paths ===
PROJECT_ROOT = "/mnt/t2-6tb/Linpeikai/Voice/ATRI"
GPT_SOVITS_PATH = f"{PROJECT_ROOT}/frameworks/GPT-SoVITS"
REFERENCE_LIBRARY = f"{PROJECT_ROOT}/dataset/reference_library.json"
OUTPUT_DIR = f"{PROJECT_ROOT}/tts_outputs/full_pipeline"

# çº¯è¡€ v4 æ¨¡å‹ (GPT e20 + SoVITS e10)
GPT_MODEL = f"{GPT_SOVITS_PATH}/GPT_weights_v2/ATRI-e20.ckpt"
SOVITS_MODEL = f"{GPT_SOVITS_PATH}/SoVITS_weights_v4/ATRI_e10_s910_l32.pth"

# LLM é…ç½®
LLM_MODEL_PATH = f"{PROJECT_ROOT}/weights/llm/Qwen2.5-14B-Roleplay-ZH"

sys.path.insert(0, GPT_SOVITS_PATH)
sys.path.insert(0, os.path.join(GPT_SOVITS_PATH, "GPT_SoVITS"))
os.chdir(GPT_SOVITS_PATH)

# === æƒ…æ„Ÿ-å‚æ•°æ˜ å°„ ===
EMOTION_PARAMS = {
    "happy": {"speed": 1.05, "top_k": 5, "temperature": 0.6},
    "proud": {"speed": 1.0, "top_k": 5, "temperature": 0.5},
    "shy": {"speed": 0.92, "top_k": 5, "temperature": 0.55},
    "sad": {"speed": 0.85, "top_k": 3, "temperature": 0.45},
    "normal": {"speed": 0.95, "top_k": 5, "temperature": 0.5},
    "love": {"speed": 0.9, "top_k": 5, "temperature": 0.55},
}

def load_reference_library():
    """åŠ è½½å‚è€ƒéŸ³é¢‘åº“"""
    with open(REFERENCE_LIBRARY, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get("recommended", {})

def analyze_emotion_with_llm(text: str) -> dict:
    """ä½¿ç”¨ LLM åˆ†ææ–‡æœ¬æƒ…æ„Ÿå’Œåˆæˆå‚æ•°"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print("ğŸ§  åŠ è½½ LLM...")
        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="cuda:1",  # ä½¿ç”¨ç©ºé—²çš„ GPU 1
            trust_remote_code=True
        )
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚åˆ†æä»¥ä¸‹äºšæ‰˜è‰è§’è‰²çš„å°è¯ï¼Œè¾“å‡ºæƒ…æ„Ÿæ ‡ç­¾å’Œè¯­éŸ³åˆæˆå‚æ•°ã€‚

å°è¯: {text}

è¯·ç”¨ä»¥ä¸‹JSONæ ¼å¼å›å¤:
{{"emotion": "happy/proud/shy/sad/normal/love", "speed": 0.8-1.1, "reason": "ç®€çŸ­ç†ç”±"}}

åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹:"""

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.3)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå– JSON
        json_match = re.search(r'\{[^}]+\}', response)
        if json_match:
            result = json.loads(json_match.group())
            return result
    except Exception as e:
        print(f"âš ï¸ LLM åˆ†æå¤±è´¥: {e}")
    
    # å›é€€ï¼šåŸºäºå…³é”®è¯çš„ç®€å•åˆ†æ
    return analyze_emotion_simple(text)

def analyze_emotion_simple(text: str) -> dict:
    """ç®€å•å…³é”®è¯æƒ…æ„Ÿåˆ†æï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    keywords = {
        "happy": ["å¬‰ã—ã„", "æ¥½ã—ã„", "ã‚„ã£ãŸ", "å¤§å¥½ã", "å¥½ã"],
        "proud": ["é«˜æ€§èƒ½", "å½“ç„¶", "ä»»ã›ã¦", "ã§ãã¾ã™"],
        "shy": ["æ¥ãšã‹ã—", "ãˆã£ã¨", "ãã®"],
        "sad": ["æ‚²ã—ã„", "å¯‚ã—ã„", "ã”ã‚ã‚“"],
        "love": ["æ„›ã—ã¦", "å¥½ãã§ã™", "ãƒ‡ãƒ¼ãƒˆ"],
    }
    
    for emotion, words in keywords.items():
        if any(w in text for w in words):
            return {"emotion": emotion, "speed": EMOTION_PARAMS[emotion]["speed"]}
    
    return {"emotion": "normal", "speed": 0.95}

def synthesize_with_v4(text: str, ref_audio: dict, params: dict, output_path: str):
    """ä½¿ç”¨ v4 æ¨¡å‹åˆæˆè¯­éŸ³"""
    from GPT_SoVITS.inference_webui import change_gpt_weights, change_sovits_weights, get_tts_wav
    from tools.i18n.i18n import I18nAuto
    i18n = I18nAuto()
    
    print(f"ğŸ”§ åŠ è½½ v4 æ¨¡å‹...")
    change_gpt_weights(GPT_MODEL)
    for _ in change_sovits_weights(SOVITS_MODEL, prompt_language="æ—¥æ–‡", text_language="æ—¥æ–‡"):
        pass
    
    speed = params.get("speed", 0.95)
    top_k = params.get("top_k", 5)
    temperature = params.get("temperature", 0.5)
    
    print(f"ğŸ¤ åˆæˆä¸­... (speed={speed}, top_k={top_k}, temp={temperature})")
    
    synthesis_result = get_tts_wav(
        ref_wav_path=ref_audio["path"],
        prompt_text=ref_audio["text"],
        prompt_language=i18n("æ—¥æ–‡"),
        text=text,
        text_language=i18n("æ—¥æ–‡"),
        how_to_cut=i18n("å‡‘å››å¥ä¸€åˆ‡"),
        top_k=top_k,
        top_p=0.8,
        temperature=temperature,
        speed=speed,
    )
    
    result_list = list(synthesis_result)
    if result_list:
        sr = result_list[0][0]
        audio = np.concatenate([item[1] for item in result_list])
        sf.write(output_path, audio, sr)
        return True
    return False

def main():
    parser = argparse.ArgumentParser(description="ATRI å…¨é“¾è·¯ TTS")
    parser.add_argument("--text", type=str, required=True, help="è¦åˆæˆçš„æ–‡æœ¬")
    parser.add_argument("--skip-llm", action="store_true", help="è·³è¿‡ LLM åˆ†æï¼Œä½¿ç”¨ç®€å•å…³é”®è¯")
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¯ ATRI å…¨é“¾è·¯è¯­éŸ³åˆæˆ")
    print("=" * 60)
    print(f"ğŸ“ è¾“å…¥: {args.text}")
    
    # Step 1: æƒ…æ„Ÿåˆ†æ
    print("\nğŸ” Step 1: æƒ…æ„Ÿåˆ†æ")
    if args.skip_llm:
        emotion_result = analyze_emotion_simple(args.text)
    else:
        emotion_result = analyze_emotion_with_llm(args.text)
    
    emotion = emotion_result.get("emotion", "normal")
    print(f"   â†’ æ£€æµ‹æƒ…æ„Ÿ: [{emotion.upper()}]")
    
    # Step 2: é€‰æ‹©å‚è€ƒéŸ³é¢‘
    print("\nğŸ“‚ Step 2: é€‰æ‹©å‚è€ƒéŸ³é¢‘")
    ref_lib = load_reference_library()
    refs = ref_lib.get(emotion, ref_lib.get("normal", []))
    
    if not refs:
        print("   âš ï¸ æœªæ‰¾åˆ°å‚è€ƒéŸ³é¢‘ï¼Œä½¿ç”¨é»˜è®¤")
        refs = list(ref_lib.values())[0] if ref_lib else []
    
    ref_audio = random.choice(refs) if refs else None
    if ref_audio:
        print(f"   â†’ å‚è€ƒ: {ref_audio['text'][:30]}...")
    
    # Step 3: åˆæˆ
    print("\nğŸ¤ Step 3: v4 è¯­éŸ³åˆæˆ")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"{OUTPUT_DIR}/atri_{emotion}_{timestamp}.wav"
    
    params = EMOTION_PARAMS.get(emotion, EMOTION_PARAMS["normal"])
    
    if ref_audio and synthesize_with_v4(args.text, ref_audio, params, output_path):
        print(f"\nâœ… ç”ŸæˆæˆåŠŸ: {output_path}")
    else:
        print("\nâŒ åˆæˆå¤±è´¥")

if __name__ == "__main__":
    main()
